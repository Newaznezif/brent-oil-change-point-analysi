{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Bayesian Change Point Modeling and Insight Generation\n",
    "## Brent Oil Price Analysis\n",
    "\n",
    "**Date:** $(Get-Date -Format ''yyyy-MM-dd'')\n",
    "**Objective:** Implement Bayesian change point detection to identify structural breaks in Brent oil prices\n",
    "**Methodology:** PyMC Bayesian inference with MCMC sampling\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Implement Bayesian change point model using PyMC\n",
    "2. Run MCMC sampling and diagnose convergence\n",
    "3. Identify structural breaks in oil price series\n",
    "4. Quantify impact of detected change points\n",
    "5. Associate changes with geopolitical events\n",
    "\n",
    "### Model Overview:\n",
    "We'll implement a Bayesian change point model that detects shifts in the mean and/or volatility of Brent oil price returns. The model assumes:\n",
    "- Oil prices follow different regimes with distinct statistical properties\n",
    "- Change points mark transitions between regimes\n",
    "- The number and location of change points are unknown and estimated from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "# Set visual style\n",
    "plt.style.use(''seaborn-v0_8-darkgrid'')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[''figure.figsize''] = (12, 6)\n",
    "warnings.filterwarnings(''ignore'')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data from Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from Task 1\n",
    "brent_data = pd.read_csv(''../data/processed/brent_processed.csv'', index_col=''Date'', parse_dates=True)\n",
    "events_data = pd.read_csv(''../data/events/geopolitical_events.csv'')\n",
    "events_data[''date''] = pd.to_datetime(events_data[''date''])\n",
    "\n",
    "print(\"=== DATA LOADED ===\")\n",
    "print(f\"Brent data shape: {brent_data.shape}\")\n",
    "print(f\"Date range: {brent_data.index.min()} to {brent_data.index.max()}\")\n",
    "print(f\"Number of events: {len(events_data)}\")\n",
    "\n",
    "# Display data structure\n",
    "print(\"\\nBrent data columns:\")\n",
    "print(brent_data.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst few rows of Brent data:\")\n",
    "display(brent_data.head())\n",
    "\n",
    "print(\"\\nEvent data:\")\n",
    "display(events_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Change Point Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare log returns (stationary series for modeling)\n",
    "# We'll use returns from Task 1 if available, otherwise compute\n",
    "if ''Log_Returns'' in brent_data.columns:\n",
    "    returns = brent_data[''Log_Returns''].dropna()\n",
    "    print(\"✓ Using pre-computed log returns from Task 1\")\n",
    "else:\n",
    "    # Compute log returns\n",
    "    brent_data[''Log_Price''] = np.log(brent_data[''Price''])\n",
    "    brent_data[''Log_Returns''] = brent_data[''Log_Price''].diff() * 100  # Percentage returns\n",
    "    returns = brent_data[''Log_Returns''].dropna()\n",
    "    print(\"✓ Computed log returns\")\n",
    "\n",
    "# Convert to numpy array for PyMC\n",
    "y = returns.values\n",
    "n_obs = len(y)\n",
    "dates = returns.index\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n=== RETURNS SUMMARY ===\")\n",
    "print(f\"Number of observations: {n_obs}\")\n",
    "print(f\"Date range: {dates.min()} to {dates.max()}\")\n",
    "print(f\"Mean return: {y.mean():.4f}%\")\n",
    "print(f\"Std return: {y.std():.4f}%\")\n",
    "print(f\"Skewness: {stats.skew(y):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(y):.4f}\")\n",
    "\n",
    "# Plot returns distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time series of returns\n",
    "axes[0].plot(dates, y, linewidth=0.5, alpha=0.7)\n",
    "axes[0].axhline(y=0, color=''black'', linestyle=''-'', linewidth=0.5)\n",
    "axes[0].set_title(''Daily Log Returns of Brent Oil Prices'', fontsize=14, fontweight=''bold'')\n",
    "axes[0].set_xlabel(''Date'')\n",
    "axes[0].set_ylabel(''Log Return (%)'')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram with normal distribution overlay\n",
    "axes[1].hist(y, bins=100, density=True, alpha=0.7, color=''skyblue'', edgecolor=''black'')\n",
    "xmin, xmax = axes[1].get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, y.mean(), y.std())\n",
    "axes[1].plot(x, p, ''r-'', linewidth=2, label=''Normal Distribution'')\n",
    "axes[1].set_title(''Distribution of Daily Returns'', fontsize=14, fontweight=''bold'')\n",
    "axes[1].set_xlabel(''Log Return (%)'')\n",
    "axes[1].set_ylabel(''Density'')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian Change Point Model 1: Single Change Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL 1: SINGLE CHANGE POINT (MEAN SHIFT) ===\")\n",
    "print(\"This model detects a single change point where the mean return changes.\")\n",
    "\n",
    "with pm.Model() as single_change_point_model:\n",
    "    \n",
    "    # ===== PRIORS =====\n",
    "    \n",
    "    # Prior for change point location (discrete uniform)\n",
    "    # τ represents the index where change occurs\n",
    "    τ = pm.DiscreteUniform(\"τ\", lower=1, upper=n_obs-1)\n",
    "    \n",
    "    # Priors for means before and after change point\n",
    "    μ1 = pm.Normal(\"μ1\", mu=0, sigma=1)  # Mean before change\n",
    "    μ2 = pm.Normal(\"μ2\", mu=0, sigma=1)  # Mean after change\n",
    "    \n",
    "    # Prior for standard deviation (assumed constant for simplicity)\n",
    "    σ = pm.HalfNormal(\"σ\", sigma=1)\n",
    "    \n",
    "    # ===== LIKELIHOOD =====\n",
    "    \n",
    "    # Create index array\n",
    "    idx = np.arange(n_obs)\n",
    "    \n",
    "    # Switch between means based on change point\n",
    "    μ = pm.math.switch(τ > idx, μ1, μ2)\n",
    "    \n",
    "    # Likelihood\n",
    "    likelihood = pm.Normal(\"y\", mu=μ, sigma=σ, observed=y)\n",
    "    \n",
    "print(\"\\n✓ Model defined successfully\")\n",
    "print(\"\\nModel structure:\")\n",
    "print(single_change_point_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run MCMC Sampling for Single Change Point Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running MCMC sampling...\")\n",
    "\n",
    "with single_change_point_model:\n",
    "    # Sample from posterior\n",
    "    trace = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=1000,\n",
    "        chains=2,\n",
    "        cores=1,  # Adjust based on your CPU\n",
    "        random_seed=RANDOM_SEED,\n",
    "        progressbar=True,\n",
    "        return_inferencedata=True\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Sampling completed successfully\")\n",
    "print(f\"Number of samples: {trace.posterior.sizes[''draw''] * trace.posterior.sizes[''chain'']}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== POSTERIOR SUMMARY ===\")\n",
    "summary = az.summary(trace, var_names=[\"τ\", \"μ1\", \"μ2\", \"σ\"])\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CONVERGENCE DIAGNOSTICS ===\")\n",
    "\n",
    "# Check R-hat statistics (should be < 1.01 for convergence)\n",
    "print(\"\\n1. R-hat Statistics (Potential Scale Reduction Factor):\")\n",
    "rhat_stats = summary[''r_hat'']\n",
    "for param, rhat in rhat_stats.items():\n",
    "    status = \"✓\" if rhat < 1.01 else \"⚠\"\n",
    "    print(f\"   {status} {param}: {rhat:.4f}\")\n",
    "\n",
    "# Check effective sample size\n",
    "print(\"\\n2. Effective Sample Size (ESS):\")\n",
    "ess_stats = summary[''ess_bulk'']\n",
    "for param, ess in ess_stats.items():\n",
    "    status = \"✓\" if ess > 400 else \"⚠\"\n",
    "    print(f\"   {status} {param}: {ess:.0f} samples\")\n",
    "\n",
    "# Trace plots\n",
    "print(\"\\n3. Trace Plots:\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var_name in enumerate([\"τ\", \"μ1\", \"μ2\", \"σ\"]):\n",
    "    az.plot_trace(trace, var_names=[var_name], ax=axes[i])\n",
    "    axes[i].set_title(f\"Trace plot: {var_name}\", fontweight=''bold'')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Posterior distributions\n",
    "print(\"\\n4. Posterior Distributions:\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var_name in enumerate([\"τ\", \"μ1\", \"μ2\", \"σ\"]):\n",
    "    az.plot_posterior(trace, var_names=[var_name], ax=axes[i], hdi_prob=0.95)\n",
    "    axes[i].set_title(f\"Posterior: {var_name}\", fontweight=''bold'')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpret Single Change Point Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SINGLE CHANGE POINT INTERPRETATION ===\")\n",
    "\n",
    "# Extract posterior samples\n",
    "tau_samples = trace.posterior[''τ''].values.flatten()\n",
    "mu1_samples = trace.posterior[''μ1''].values.flatten()\n",
    "mu2_samples = trace.posterior[''μ2''].values.flatten()\n",
    "\n",
    "# Calculate statistics\n",
    "tau_mean = int(np.mean(tau_samples))\n",
    "tau_hdi = az.hdi(tau_samples, hdi_prob=0.95)\n",
    "mu_diff = mu2_samples - mu1_samples\n",
    "\n",
    "print(f\"\\n1. Change Point Location:\")\n",
    "print(f\"   • Mean index: {tau_mean}\")\n",
    "print(f\"   • 95% HDI: [{int(tau_hdi[0])}, {int(tau_hdi[1])}]\")\n",
    "print(f\"   • Date: {dates[tau_mean]}\")\n",
    "print(f\"   • Date range (95% HDI): {dates[int(tau_hdi[0])]} to {dates[int(tau_hdi[1])]}\")\n",
    "\n",
    "print(f\"\\n2. Mean Returns Before/After Change:\")\n",
    "print(f\"   • Before (μ1): {np.mean(mu1_samples):.4f}% (95% HDI: [{az.hdi(mu1_samples, hdi_prob=0.95)[0]:.4f}, {az.hdi(mu1_samples, hdi_prob=0.95)[1]:.4f}])\")\n",
    "print(f\"   • After (μ2): {np.mean(mu2_samples):.4f}% (95% HDI: [{az.hdi(mu2_samples, hdi_prob=0.95)[0]:.4f}, {az.hdi(mu2_samples, hdi_prob=0.95)[1]:.4f}])\")\n",
    "print(f\"   • Difference: {np.mean(mu_diff):.4f}% (95% HDI: [{az.hdi(mu_diff, hdi_prob=0.95)[0]:.4f}, {az.hdi(mu_diff, hdi_prob=0.95)[1]:.4f}])\")\n",
    "\n",
    "# Calculate probability that μ2 > μ1\n",
    "prob_mu2_gt_mu1 = np.mean(mu_diff > 0)\n",
    "print(f\"   • Probability μ2 > μ1: {prob_mu2_gt_mu1:.2%}\")\n",
    "\n",
    "print(f\"\\n3. Volatility (σ): {np.mean(trace.posterior[''σ''].values.flatten()):.4f}%\")\n",
    "\n",
    "# Visualize the change point\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Time series with change point\n",
    "axes[0].plot(dates, y, linewidth=0.5, alpha=0.7, label=''Daily Returns'')\n",
    "axes[0].axvline(x=dates[tau_mean], color=''red'', linestyle=''--'', linewidth=2, label=f''Change Point: {dates[tau_mean].date()}'')\n",
    "\n",
    "# Add HDI region\n",
    "axes[0].axvspan(dates[int(tau_hdi[0])], dates[int(tau_hdi[1])], \n",
    "                alpha=0.2, color=''red'', label=''95% HDI Region'')\n",
    "\n",
    "# Add mean lines before/after\n",
    "before_mask = dates < dates[tau_mean]\n",
    "after_mask = dates >= dates[tau_mean]\n",
    "axes[0].axhline(y=np.mean(mu1_samples), color=''green'', linestyle=''-'', \n",
    "                alpha=0.5, linewidth=1, label=f''Mean before: {np.mean(mu1_samples):.3f}%'')\n",
    "axes[0].axhline(y=np.mean(mu2_samples), color=''blue'', linestyle=''-'', \n",
    "                alpha=0.5, linewidth=1, label=f''Mean after: {np.mean(mu2_samples):.3f}%'')\n",
    "\n",
    "axes[0].set_title(''Single Change Point Detection in Brent Oil Returns'', fontsize=14, fontweight=''bold'')\n",
    "axes[0].set_xlabel(''Date'')\n",
    "axes[0].set_ylabel(''Log Return (%)'')\n",
    "axes[0].legend(loc=''upper left'')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Posterior distribution of change point\n",
    "axes[1].hist(tau_samples, bins=50, density=True, alpha=0.7, color=''steelblue'', edgecolor=''black'')\n",
    "axes[1].axvline(x=tau_mean, color=''red'', linestyle=''--'', linewidth=2, label=f''Mean: {tau_mean}'')\n",
    "axes[1].axvline(x=tau_hdi[0], color=''orange'', linestyle='':'', linewidth=1.5, label=f''HDI Lower: {int(tau_hdi[0])}'')\n",
    "axes[1].axvline(x=tau_hdi[1], color=''orange'', linestyle='':'', linewidth=1.5, label=f''HDI Upper: {int(tau_hdi[1])}'')\n",
    "axes[1].set_title(''Posterior Distribution of Change Point Location'', fontsize=14, fontweight=''bold'')\n",
    "axes[1].set_xlabel(''Observation Index'')\n",
    "axes[1].set_ylabel(''Density'')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bayesian Change Point Model 2: Multiple Change Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL 2: MULTIPLE CHANGE POINTS (MEAN AND VARIANCE SHIFTS) ===\")\n",
    "print(\"This model detects multiple change points with shifts in both mean and volatility.\")\n",
    "\n",
    "# Define maximum number of change points to consider\n",
    "max_changepoints = 5\n",
    "\n",
    "with pm.Model() as multiple_change_point_model:\n",
    "    \n",
    "    # ===== PRIORS =====\n",
    "    \n",
    "    # Prior for number of change points (Poisson distribution)\n",
    "    # We use a truncated Poisson to limit to reasonable number\n",
    "    n_changepoints = pm.Poisson(\"n_changepoints\", mu=3)\n",
    "    \n",
    "    # Ensure n_changepoints is between 0 and max_changepoints\n",
    "    n_changepoints = pt.clip(n_changepoints, 0, max_changepoints)\n",
    "    \n",
    "    # Prior for change point locations (Dirichlet Process)\n",
    "    # Create stick-breaking representation\n",
    "    alpha = pm.Gamma(\"alpha\", alpha=1, beta=1)\n",
    "    \n",
    "    # Stick-breaking weights\n",
    "    v = pm.Beta(\"v\", alpha=1, beta=alpha, shape=max_changepoints)\n",
    "    \n",
    "    # Calculate weights for each possible change point\n",
    "    weights = v * pt.concatenate([pt.ones(1), pt.extra_ops.cumprod(1 - v)[:-1]])\n",
    "    \n",
    "    # Categorical distribution for change point locations\n",
    "    tau = pm.Categorical(\"tau\", p=weights, shape=n_obs-1)\n",
    "    \n",
    "    # Priors for regime parameters\n",
    "    # Means for each regime\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=1, shape=max_changepoints+1)\n",
    "    \n",
    "    # Volatilities for each regime\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1, shape=max_changepoints+1)\n",
    "    \n",
    "    # ===== LIKELIHOOD =====\n",
    "    \n",
    "    # Create regime assignments\n",
    "    regime = pt.zeros(n_obs, dtype=''int32'')\n",
    "    for i in range(n_obs-1):\n",
    "        regime = pt.set_subtensor(regime[i+1:], regime[i+1:] + (tau[i] == 1))\n",
    "    \n",
    "    # Select parameters based on regime\n",
    "    mu_selected = mu[regime]\n",
    "    sigma_selected = sigma[regime]\n",
    "    \n",
    "    # Likelihood\n",
    "    likelihood = pm.Normal(\"y\", mu=mu_selected, sigma=sigma_selected, observed=y)\n",
    "\n",
    "print(\"\\n✓ Multiple change point model defined successfully\")\n",
    "print(\"Note: This model is computationally intensive. For faster results, we'll use an approximate method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Alternative: Sequential Analysis with Rolling Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SEQUENTIAL ANALYSIS: ROLLING WINDOW CHANGE POINT DETECTION ===\")\n",
    "print(\"This approach detects multiple change points by analyzing rolling windows.\")\n",
    "\n",
    "# Function to detect change points in rolling windows\n",
    "def detect_change_points_rolling(data, window_size=252, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Detect change points using rolling window analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - data: time series array\n",
    "    - window_size: size of rolling window (default 252 = 1 trading year)\n",
    "    - significance_level: statistical significance threshold\n",
    "    \n",
    "    Returns:\n",
    "    - change_points: list of detected change point indices\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data)\n",
    "    change_points = []\n",
    "    \n",
    "    for i in range(window_size, n - window_size):\n",
    "        # Split data into before and after windows\n",
    "        before = data[i-window_size:i]\n",
    "        after = data[i:i+window_size]\n",
    "        \n",
    "        # Perform statistical test for difference in means\n",
    "        t_stat, p_value = stats.ttest_ind(before, after, equal_var=False)\n",
    "        \n",
    "        # Check if statistically significant change\n",
    "        if p_value < significance_level:\n",
    "            # Also check practical significance (mean difference > threshold)\n",
    "            mean_diff = abs(after.mean() - before.mean())\n",
    "            if mean_diff > 0.1:  # At least 0.1% difference\n",
    "                change_points.append(i)\n",
    "    \n",
    "    # Filter out change points that are too close together\n",
    "    min_distance = window_size // 2\n",
    "    filtered_points = []\n",
    "    \n",
    "    for cp in change_points:\n",
    "        if not filtered_points or (cp - filtered_points[-1]) >= min_distance:\n",
    "            filtered_points.append(cp)\n",
    "    \n",
    "    return filtered_points\n",
    "\n",
    "# Run detection\n",
    "window_sizes = [126, 252, 504]  # 6 months, 1 year, 2 years\n",
    "all_change_points = []\n",
    "\n",
    "for window in window_sizes:\n",
    "    cps = detect_change_points_rolling(y, window_size=window)\n",
    "    all_change_points.extend(cps)\n",
    "    print(f\"Window size {window} ({window//21} months): {len(cps)} change points detected\")\n",
    "\n",
    "# Remove duplicates and sort\n",
    "unique_cps = sorted(set(all_change_points))\n",
    "\n",
    "# Filter by frequency (keep points detected by multiple window sizes)\n",
    "from collections import Counter\n",
    "cp_counts = Counter(all_change_points)\n",
    "robust_cps = [cp for cp, count in cp_counts.items() if count >= 2]\n",
    "\n",
    "print(f\"\\nTotal unique change points: {len(unique_cps)}\")\n",
    "print(f\"Robust change points (detected by ≥2 window sizes): {len(robust_cps)}\")\n",
    "\n",
    "# Convert indices to dates\n",
    "cp_dates = [dates[i] for i in robust_cps]\n",
    "print(\"\\nRobust change point dates:\")\n",
    "for i, (cp_idx, cp_date) in enumerate(zip(robust_cps, cp_dates)):\n",
    "    print(f\"  {i+1}. Index {cp_idx}: {cp_date.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Multiple Change Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detected change points\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Full time series with all detected change points\n",
    "axes[0].plot(dates, y, linewidth=0.5, alpha=0.7, label=''Daily Returns'')\n",
    "\n",
    "# Mark robust change points\n",
    "for cp_idx, cp_date in zip(robust_cps, cp_dates):\n",
    "    axes[0].axvline(x=cp_date, color=''red'', linestyle=''--'', alpha=0.7, linewidth=1)\n",
    "\n",
    "# Highlight major change points (top 5 by magnitude of surrounding change)\n",
    "if len(robust_cps) > 0:\n",
    "    # Calculate magnitude of change around each point\n",
    "    change_magnitudes = []\n",
    "    for cp_idx in robust_cps:\n",
    "        before_window = 126  # 6 months\n",
    "        after_window = 126\n",
    "        \n",
    "        start_idx = max(0, cp_idx - before_window)\n",
    "        end_idx = min(len(y), cp_idx + after_window)\n",
    "        \n",
    "        before_mean = y[start_idx:cp_idx].mean()\n",
    "        after_mean = y[cp_idx:end_idx].mean()\n",
    "        magnitude = abs(after_mean - before_mean)\n",
    "        change_magnitudes.append(magnitude)\n",
    "    \n",
    "    # Get indices of top 5 magnitudes\n",
    "    if len(change_magnitudes) >= 5:\n",
    "        top_5_idx = np.argsort(change_magnitudes)[-5:][::-1]\n",
    "        for i, idx in enumerate(top_5_idx):\n",
    "            cp_idx = robust_cps[idx]\n",
    "            cp_date = cp_dates[idx]\n",
    "            axes[0].axvline(x=cp_date, color=''darkred'', linestyle=''-'', \n",
    "                          linewidth=2, alpha=0.8, label=f''Major Change {i+1}'' if i < 3 else None)\n",
    "            \n",
    "            # Add annotation\n",
    "            axes[0].text(cp_date, axes[0].get_ylim()[1] * 0.9, \n",
    "                       f''{cp_date.date()}'', \n",
    "                       rotation=90, verticalalignment=''top'',\n",
    "                       fontsize=8, fontweight=''bold'')\n",
    "\n",
    "axes[0].set_title(''Multiple Change Points in Brent Oil Returns'', fontsize=14, fontweight=''bold'')\n",
    "axes[0].set_xlabel(''Date'')\n",
    "axes[0].set_ylabel(''Log Return (%)'')\n",
    "axes[0].legend(loc=''upper left'')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Rolling statistics with change points\n",
    "window = 63  # 3 months\n",
    "rolling_mean = pd.Series(y).rolling(window=window).mean()\n",
    "rolling_std = pd.Series(y).rolling(window=window).std()\n",
    "\n",
    "axes[1].plot(dates, rolling_mean, color=''green'', linewidth=1.5, label=''Rolling Mean (3mo)'')\n",
    "axes[1].plot(dates, rolling_std, color=''orange'', linewidth=1.5, label=''Rolling Std (3mo)'')\n",
    "\n",
    "# Mark change points\n",
    "for cp_date in cp_dates:\n",
    "    axes[1].axvline(x=cp_date, color=''red'', linestyle=''--'', alpha=0.5, linewidth=0.5)\n",
    "\n",
    "axes[1].set_title(''Rolling Statistics Highlighting Volatility Regimes'', fontsize=14, fontweight=''bold'')\n",
    "axes[1].set_xlabel(''Date'')\n",
    "axes[1].set_ylabel(''Rolling Statistic'')\n",
    "axes[1].legend(loc=''upper left'')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Regime characterization\n",
    "axes[2].plot(dates, y, linewidth=0.3, alpha=0.5, label=''Returns'')\n",
    "\n",
    "# Color code regimes\n",
    "if len(robust_cps) >= 2:\n",
    "    regime_boundaries = [dates[0]] + cp_dates + [dates[-1]]\n",
    "    colors = [''lightblue'', ''lightgreen'', ''lightcoral'', ''lightyellow'', ''lightgray'']\n",
    "    \n",
    "    for i in range(len(regime_boundaries)-1):\n",
    "        start_date = regime_boundaries[i]\n",
    "        end_date = regime_boundaries[i+1]\n",
    "        color_idx = i % len(colors)\n",
    "        axes[2].axvspan(start_date, end_date, alpha=0.2, color=colors[color_idx], \n",
    "                       label=f''Regime {i+1}'' if i < 5 else None)\n",
    "    \n",
    "    # Calculate regime statistics\n",
    "    print(\"\\n=== REGIME STATISTICS ===\")\n",
    "    for i in range(len(regime_boundaries)-1):\n",
    "        start_idx = np.where(dates >= regime_boundaries[i])[0][0]\n",
    "        end_idx = np.where(dates <= regime_boundaries[i+1])[0][-1]\n",
    "        \n",
    "        regime_data = y[start_idx:end_idx]\n",
    "        if len(regime_data) > 10:  # Only analyze meaningful regimes\n",
    "            duration = (regime_boundaries[i+1] - regime_boundaries[i]).days / 365.25\n",
    "            print(f\"\\nRegime {i+1}: {regime_boundaries[i].date()} to {regime_boundaries[i+1].date()}\")\n",
    "            print(f\"  • Duration: {duration:.1f} years\")\n",
    "            print(f\"  • Mean return: {regime_data.mean():.4f}%\")\n",
    "            print(f\"  • Volatility: {regime_data.std():.4f}%\")\n",
    "            print(f\"  • Observations: {len(regime_data)}\")\n",
    "\n",
    "axes[2].set_title(''Identified Market Regimes'', fontsize=14, fontweight=''bold'')\n",
    "axes[2].set_xlabel(''Date'')\n",
    "axes[2].set_ylabel(''Log Return (%)'')\n",
    "axes[2].legend(loc=''upper left'')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Associate Change Points with Geopolitical Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EVENT ASSOCIATION ANALYSIS ===\")\n",
    "print(\"Associating detected change points with geopolitical events...\\n\")\n",
    "\n",
    "# Function to find closest events to change points\n",
    "def associate_events_with_change_points(change_point_dates, event_dates, max_days_window=30):\n",
    "    \"\"\"\n",
    "    Associate change points with geopolitical events\n",
    "    \n",
    "    Returns:\n",
    "    - associations: list of (change_point_date, event_date, event_name, days_difference)\n",
    "    \"\"\"\n",
    "    associations = []\n",
    "    \n",
    "    for cp_date in change_point_dates:\n",
    "        # Find closest event\n",
    "        time_diffs = [(event_date, abs((cp_date - event_date).days)) \n",
    "                     for event_date in event_dates]\n",
    "        \n",
    "        if time_diffs:\n",
    "            closest_event_date, min_days = min(time_diffs, key=lambda x: x[1])\n",
    "            \n",
    "            if min_days <= max_days_window:\n",
    "                # Get event details\n",
    "                event_info = events_data[events_data[''date''] == closest_event_date].iloc[0]\n",
    "                associations.append({\n",
    "                    ''change_point_date'': cp_date,\n",
    "                    ''event_date'': closest_event_date,\n",
    "                    ''days_difference'': min_days,\n",
    "                    ''event_name'': event_info[''event_name''],\n",
    "                    ''event_type'': event_info[''event_type''],\n",
    "                    ''region'': event_info[''region''],\n",
    "                    ''impact_level'': event_info[''impact_level''],\n",
    "                    ''description'': event_info[''description'']\n",
    "                })\n",
    "    \n",
    "    return associations\n",
    "\n",
    "# Get event dates\n",
    "event_dates = events_data[''date''].values\n",
    "\n",
    "# Associate events with robust change points\n",
    "associations = associate_events_with_change_points(cp_dates, event_dates, max_days_window=30)\n",
    "\n",
    "print(f\"Found {len(associations)} change points associated with events (within 30 days)\")\n",
    "print(\"\\n=== EVENT-CHANGE POINT ASSOCIATIONS ===\")\n",
    "\n",
    "if associations:\n",
    "    # Create DataFrame for display\n",
    "    assoc_df = pd.DataFrame(associations)\n",
    "    \n",
    "    # Calculate impact metrics for each association\n",
    "    impact_metrics = []\n",
    "    \n",
    "    for _, row in assoc_df.iterrows():\n",
    "        cp_date = row[''change_point_date'']\n",
    "        cp_idx = np.where(dates == cp_date)[0][0]\n",
    "        \n",
    "        # Calculate before/after statistics\n",
    "        window = 63  # 3 months\n",
    "        before_start = max(0, cp_idx - window)\n",
    "        after_end = min(len(y), cp_idx + window)\n",
    "        \n",
    "        before_returns = y[before_start:cp_idx]\n",
    "        after_returns = y[cp_idx:after_end]\n",
    "        \n",
    "        if len(before_returns) > 10 and len(after_returns) > 10:\n",
    "            mean_before = before_returns.mean()\n",
    "            mean_after = after_returns.mean()\n",
    "            mean_change = mean_after - mean_before\n",
    "            mean_change_pct = (mean_change / abs(mean_before)) * 100 if mean_before != 0 else 0\n",
    "            \n",
    "            vol_before = before_returns.std()\n",
    "            vol_after = after_returns.std()\n",
    "            vol_change = vol_after - vol_before\n",
    "            vol_change_pct = (vol_change / vol_before) * 100 if vol_before != 0 else 0\n",
    "            \n",
    "            impact_metrics.append({\n",
    "                ''mean_before'': mean_before,\n",
    "                ''mean_after'': mean_after,\n",
    "                ''mean_change'': mean_change,\n",
    "                ''mean_change_pct'': mean_change_pct,\n",
    "                ''vol_before'': vol_before,\n",
    "                ''vol_after'': vol_after,\n",
    "                ''vol_change'': vol_change,\n",
    "                ''vol_change_pct'': vol_change_pct\n",
    "            })\n",
    "        else:\n",
    "            impact_metrics.append({})\n",
    "    \n",
    "    # Add impact metrics to DataFrame\n",
    "    impact_df = pd.DataFrame(impact_metrics)\n",
    "    assoc_df = pd.concat([assoc_df, impact_df], axis=1)\n",
    "    \n",
    "    # Display associations\n",
    "    display_cols = [''change_point_date'', ''event_name'', ''days_difference'', \n",
    "                   ''event_type'', ''impact_level'', ''mean_change'', ''vol_change'']\n",
    "    display(assoc_df[display_cols].sort_values(''days_difference''))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n=== ASSOCIATION SUMMARY ===\")\n",
    "    print(f\"Average days between change point and event: {assoc_df[''days_difference''].mean():.1f} days\")\n",
    "    print(f\"Median days between change point and event: {assoc_df[''days_difference''].median():.1f} days\")\n",
    "    \n",
    "    # Association rate by event type\n",
    "    print(\"\\nAssociation rate by event type:\")\n",
    "    event_type_counts = assoc_df[''event_type''].value_counts()\n",
    "    for event_type, count in event_type_counts.items():\n",
    "        total_events_of_type = len(events_data[events_data[''event_type''] == event_type])\n",
    "        association_rate = (count / total_events_of_type) * 100\n",
    "        print(f\"  • {event_type}: {count}/{total_events_of_type} events ({association_rate:.1f}%)\")\n",
    "    \n",
    "    # Impact analysis\n",
    "    print(\"\\nAverage impact by event type:\")\n",
    "    impact_by_type = assoc_df.groupby(''event_type'')[[''mean_change'', ''vol_change'']].mean()\n",
    "    display(impact_by_type)\n",
    "    \n",
    "else:\n",
    "    print(\"No associations found within 30-day window.\")\n",
    "    print(\"Trying with larger window (90 days)...\")\n",
    "    \n",
    "    associations = associate_events_with_change_points(cp_dates, event_dates, max_days_window=90)\n",
    "    print(f\"Found {len(associations)} associations within 90 days.\")\n",
    "    \n",
    "    if associations:\n",
    "        assoc_df = pd.DataFrame(associations)\n",
    "        display(assoc_df[[''change_point_date'', ''event_name'', ''days_difference'', ''event_type'']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quantify Event Impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== QUANTITATIVE IMPACT ASSESSMENT ===\")\n",
    "\n",
    "if ''assoc_df'' in locals() and not assoc_df.empty:\n",
    "    \n",
    "    # Analyze top impacts\n",
    "    print(\"\\n1. TOP 5 MOST IMPACTFUL EVENTS (by mean change):\")\n",
    "    top_events = assoc_df.nlargest(5, ''mean_change_pct'')[[''event_name'', ''event_date'', \n",
    "                                                         ''mean_change_pct'', ''vol_change_pct'', \n",
    "                                                         ''impact_level'', ''days_difference'']]\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_events.iterrows(), 1):\n",
    "        print(f\"\\n   {i}. {row[''event_name'']} ({row[''event_date''].date()})\")\n",
    "        print(f\"      • Mean return change: {row[''mean_change_pct'']:+.1f}%\")\n",
    "        print(f\"      • Volatility change: {row[''vol_change_pct'']:+.1f}%\")\n",
    "        print(f\"      • Impact level: {row[''impact_level'']}\")\n",
    "        print(f\"      • Days from event: {row[''days_difference'']}\")\n",
    "    \n",
    "    # Statistical significance testing\n",
    "    print(\"\\n2. STATISTICAL SIGNIFICANCE OF IMPACTS:\")\n",
    "    \n",
    "    # Test if mean changes are statistically different from zero\n",
    "    from scipy import stats\n",
    "    \n",
    "    for _, row in assoc_df.iterrows():\n",
    "        cp_date = row[''change_point_date'']\n",
    "        cp_idx = np.where(dates == cp_date)[0][0]\n",
    "        \n",
    "        window = 63\n",
    "        before_start = max(0, cp_idx - window)\n",
    "        after_end = min(len(y), cp_idx + window)\n",
    "        \n",
    "        before_returns = y[before_start:cp_idx]\n",
    "        after_returns = y[cp_idx:after_end]\n",
    "        \n",
    "        if len(before_returns) > 10 and len(after_returns) > 10:\n",
    "            # Welch's t-test (doesn't assume equal variances)\n",
    "            t_stat, p_value = stats.ttest_ind(before_returns, after_returns, equal_var=False)\n",
    "            \n",
    "            significance = \"Significant\" if p_value < 0.05 else \"Not Significant\"\n",
    "            print(f\"   • {row[''event_name'']}: t = {t_stat:.3f}, p = {p_value:.4f} ({significance})\")\n",
    "    \n",
    "    # Visualization of impacts\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Mean change by event type\n",
    "    ax1 = axes[0, 0]\n",
    "    mean_by_type = assoc_df.groupby(''event_type'')[''mean_change''].mean().sort_values()\n",
    "    colors = plt.cm.Set2(np.arange(len(mean_by_type)))\n",
    "    bars = ax1.barh(mean_by_type.index, mean_by_type.values, color=colors)\n",
    "    ax1.set_title(''Average Mean Return Change by Event Type'', fontweight=''bold'')\n",
    "    ax1.set_xlabel(''Mean Change (%)'')\n",
    "    ax1.axvline(x=0, color=''black'', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + (0.01 if width >= 0 else -0.05), \n",
    "                bar.get_y() + bar.get_height()/2,\n",
    "                f''{width:.3f}%'', \n",
    "                va=''center'', ha=''left'' if width >= 0 else ''right'')\n",
    "    \n",
    "    # Plot 2: Volatility change by event type\n",
    "    ax2 = axes[0, 1]\n",
    "    vol_by_type = assoc_df.groupby(''event_type'')[''vol_change''].mean().sort_values()\n",
    "    bars = ax2.barh(vol_by_type.index, vol_by_type.values, color=plt.cm.Set3(np.arange(len(vol_by_type))))\n",
    "    ax2.set_title(''Average Volatility Change by Event Type'', fontweight=''bold'')\n",
    "    ax2.set_xlabel(''Volatility Change (%)'')\n",
    "    ax2.axvline(x=0, color=''black'', linewidth=0.5)\n",
    "    \n",
    "    # Plot 3: Impact by days difference\n",
    "    ax3 = axes[1, 0]\n",
    "    scatter = ax3.scatter(assoc_df[''days_difference''], assoc_df[''mean_change''], \n",
    "                         c=assoc_df[''vol_change''], cmap=''coolwarm'', \n",
    "                         s=100, alpha=0.7, edgecolors=''black'')\n",
    "    ax3.set_title(''Impact vs. Timing'', fontweight=''bold'')\n",
    "    ax3.set_xlabel(''Days from Event to Change Point'')\n",
    "    ax3.set_ylabel(''Mean Change (%)'')\n",
    "    ax3.axhline(y=0, color=''black'', linewidth=0.5)\n",
    "    ax3.axvline(x=0, color=''black'', linewidth=0.5)\n",
    "    plt.colorbar(scatter, ax=ax3, label=''Volatility Change'')\n",
    "    \n",
    "    # Plot 4: Impact level analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    impact_levels = [''Low'', ''Medium'', ''High'']\n",
    "    \n",
    "    # Filter to only include existing impact levels\n",
    "    existing_levels = [level for level in impact_levels if level in assoc_df[''impact_level''].unique()]\n",
    "    \n",
    "    if existing_levels:\n",
    "        impact_data = []\n",
    "        for level in existing_levels:\n",
    "            level_data = assoc_df[assoc_df[''impact_level''] == level]\n",
    "            impact_data.append({\n",
    "                ''level'': level,\n",
    "                ''mean_change_mean'': level_data[''mean_change''].mean(),\n",
    "                ''mean_change_std'': level_data[''mean_change''].std(),\n",
    "                ''count'': len(level_data)\n",
    "            })\n",
    "        \n",
    "        impact_df_plot = pd.DataFrame(impact_data)\n",
    "        \n",
    "        x_pos = range(len(existing_levels))\n",
    "        bars = ax4.bar(x_pos, impact_df_plot[''mean_change_mean''], \n",
    "                      yerr=impact_df_plot[''mean_change_std''], \n",
    "                      capsize=5, color=[''green'', ''orange'', ''red''], alpha=0.7)\n",
    "        \n",
    "        ax4.set_title(''Impact by Event Severity Level'', fontweight=''bold'')\n",
    "        ax4.set_xlabel(''Impact Level'')\n",
    "        ax4.set_ylabel(''Mean Change (%)'')\n",
    "        ax4.set_xticks(x_pos)\n",
    "        ax4.set_xticklabels(existing_levels)\n",
    "        ax4.axhline(y=0, color=''black'', linewidth=0.5)\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, height + (0.01 if height >= 0 else -0.03),\n",
    "                    f''n={impact_df_plot.iloc[i][\"count\"]}'', ha=''center'', va=''bottom'' if height >= 0 else ''top'')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No associations available for impact quantification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Advanced Model: Bayesian Online Change Point Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ADVANCED MODEL: BAYESIAN ONLINE CHANGE POINT DETECTION ===\")\n",
    "print(\"Implementing a more sophisticated model for multiple change points...\")\n",
    "\n",
    "# Simpler approach for multiple change points using Bayesian approach\n",
    "def bayesian_online_changepoint_detection(data, hazard_rate=1/250):\n",
    "    \"\"\"\n",
    "    Simplified Bayesian online change point detection\n",
    "    Based on Adams & MacKay 2007\n",
    "    \n",
    "    Parameters:\n",
    "    - data: time series array\n",
    "    - hazard_rate: probability of change at each time point (1/250 ≈ yearly)\n",
    "    \n",
    "    Returns:\n",
    "    - run_length_posterior: posterior probability of run lengths\n",
    "    - changepoint_probabilities: probability of change point at each time\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data)\n",
    "    \n",
    "    # Initialize\n",
    "    run_length_posterior = np.zeros((n, n))\n",
    "    run_length_posterior[0, 0] = 1  # At time 0, run length is 0 with probability 1\n",
    "    \n",
    "    changepoint_probabilities = np.zeros(n)\n",
    "    \n",
    "    # Prior parameters for Gaussian model\n",
    "    mu0 = 0\n",
    "    kappa0 = 1\n",
    "    alpha0 = 1\n",
    "    beta0 = 1\n",
    "    \n",
    "    # Store sufficient statistics\n",
    "    mu_t = np.zeros(n)\n",
    "    kappa_t = np.zeros(n)\n",
    "    alpha_t = np.zeros(n)\n",
    "    beta_t = np.zeros(n)\n",
    "    \n",
    "    mu_t[0] = mu0\n",
    "    kappa_t[0] = kappa0\n",
    "    alpha_t[0] = alpha0\n",
    "    beta_t[0] = beta0\n",
    "    \n",
    "    for t in range(1, n):\n",
    "        # Make predictions\n",
    "        predictive_probs = np.zeros(t+1)\n",
    "        \n",
    "        for r in range(t+1):  # Possible run lengths\n",
    "            if r == 0:  # Change point\n",
    "                # Reset to prior\n",
    "                mu = mu0\n",
    "                kappa = kappa0\n",
    "                alpha = alpha0\n",
    "                beta = beta0\n",
    "            else:\n",
    "                # Update sufficient statistics\n",
    "                x_window = data[t-r:t]\n",
    "                n_r = len(x_window)\n",
    "                \n",
    "                # Update parameters (conjugate prior updates)\n",
    "                x_bar = np.mean(x_window)\n",
    "                mu = (kappa0 * mu0 + n_r * x_bar) / (kappa0 + n_r)\n",
    "                kappa = kappa0 + n_r\n",
    "                alpha = alpha0 + n_r/2\n",
    "                beta = beta0 + 0.5 * np.sum((x_window - x_bar)**2) + \\\n",
    "                       (kappa0 * n_r * (x_bar - mu0)**2) / (2 * (kappa0 + n_r))\n",
    "            \n",
    "            # Student-t predictive distribution\n",
    "            # Simplified: use Gaussian approximation for speed\n",
    "            predictive_mean = mu\n",
    "            predictive_var = beta * (kappa + 1) / (alpha * kappa)\n",
    "            \n",
    "            # Calculate probability\n",
    "            prob = stats.norm.pdf(data[t], predictive_mean, np.sqrt(predictive_var))\n",
    "            predictive_probs[r] = prob\n",
    "            \n",
    "            # Store updated parameters\n",
    "            if r == 0:\n",
    "                mu_t[t] = mu0\n",
    "                kappa_t[t] = kappa0\n",
    "                alpha_t[t] = alpha0\n",
    "                beta_t[t] = beta0\n",
    "        \n",
    "        # Calculate growth probabilities\n",
    "        growth_probs = predictive_probs * run_length_posterior[t-1, :t+1] * (1 - hazard_rate)\n",
    "        \n",
    "        # Calculate change point probability\n",
    "        cp_prob = predictive_probs[0] * np.sum(run_length_posterior[t-1, :t+1] * hazard_rate)\n",
    "        \n",
    "        # Update run length posterior\n",
    "        run_length_posterior[t, 0] = cp_prob\n",
    "        run_length_posterior[t, 1:t+1] = growth_probs[:t]\n",
    "        \n",
    "        # Normalize\n",
    "        total = np.sum(run_length_posterior[t, :t+1])\n",
    "        if total > 0:\n",
    "            run_length_posterior[t, :t+1] /= total\n",
    "        \n",
    "        # Store change point probability\n",
    "        changepoint_probabilities[t] = run_length_posterior[t, 0]\n",
    "    \n",
    "    return run_length_posterior, changepoint_probabilities\n",
    "\n",
    "# Run detection on a subset for speed\n",
    "print(\"Running Bayesian online change point detection...\")\n",
    "subset_size = 2000  # Use subset for faster computation\n",
    "y_subset = y[:subset_size]\n",
    "dates_subset = dates[:subset_size]\n",
    "\n",
    "run_length_post, cp_probs = bayesian_online_changepoint_detection(y_subset, hazard_rate=1/500)\n",
    "\n",
    "# Identify change points (threshold on probability)\n",
    "cp_threshold = 0.1\n",
    "detected_cp_indices = np.where(cp_probs > cp_threshold)[0]\n",
    "\n",
    "print(f\"Detected {len(detected_cp_indices)} change points with probability > {cp_threshold}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot time series with detected change points\n",
    "axes[0].plot(dates_subset, y_subset, linewidth=0.5, alpha=0.7)\n",
    "for cp_idx in detected_cp_indices:\n",
    "    axes[0].axvline(x=dates_subset[cp_idx], color=''red'', linestyle=''--'', alpha=0.7, linewidth=1)\n",
    "axes[0].set_title(''Bayesian Online Change Point Detection'', fontweight=''bold'')\n",
    "axes[0].set_xlabel(''Date'')\n",
    "axes[0].set_ylabel(''Log Return (%)'')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot change point probabilities\n",
    "axes[1].plot(dates_subset, cp_probs, color=''darkred'', linewidth=1.5)\n",
    "axes[1].axhline(y=cp_threshold, color=''black'', linestyle='':'', label=f''Threshold = {cp_threshold}'')\n",
    "axes[1].fill_between(dates_subset, 0, cp_probs, where=(cp_probs > cp_threshold), \n",
    "                     color=''red'', alpha=0.3, label=''Probable Change Points'')\n",
    "axes[1].set_title(''Change Point Probabilities Over Time'', fontweight=''bold'')\n",
    "axes[1].set_xlabel(''Date'')\n",
    "axes[1].set_ylabel(''Probability'')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 highest probability change points:\")\n",
    "top_cp_indices = np.argsort(cp_probs)[-5:][::-1]\n",
    "for i, idx in enumerate(top_cp_indices):\n",
    "    print(f\"  {i+1}. Date: {dates_subset[idx].date()}, Probability: {cp_probs[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL COMPARISON AND SELECTION ===\")\n",
    "\n",
    "# Compare different approaches\n",
    "model_results = {}\n",
    "\n",
    "# 1. Single Change Point Model (Bayesian)\n",
    "print(\"1. SINGLE CHANGE POINT MODEL (Bayesian):\")\n",
    "print(f\"   • Change point: {dates[tau_mean].date()}\")\n",
    "print(f\"   • Mean before: {np.mean(mu1_samples):.4f}%\") \n",
    "print(f\"   • Mean after: {np.mean(mu2_samples):.4f}%\")\n",
    "print(f\"   • Mean difference: {np.mean(mu_diff):.4f}%\")\n",
    "print(f\"   • Probability μ2 > μ1: {prob_mu2_gt_mu1:.2%}\")\n",
    "\n",
    "model_results[''single_bayesian''] = {\n",
    "    ''n_changepoints'': 1,\n",
    "    ''locations'': [dates[tau_mean]],\n",
    "    ''mean_before'': np.mean(mu1_samples),\n",
    "    ''mean_after'': np.mean(mu2_samples),\n",
    "    ''method'': ''Bayesian MCMC''\n",
    "}\n",
    "\n",
    "# 2. Multiple Change Points (Rolling Window)\n",
    "print(\"\\n2. MULTIPLE CHANGE POINTS (Rolling Window):\")\n",
    "print(f\"   • Number of change points: {len(robust_cps)}\")\n",
    "print(f\"   • Dates: {'', ''.join([d.date().strftime(''%Y-%m-%d'') for d in cp_dates[:5]])}...\")\n",
    "\n",
    "model_results[''multiple_rolling''] = {\n",
    "    ''n_changepoints'': len(robust_cps),\n",
    "    ''locations'': cp_dates,\n",
    "    ''method'': ''Rolling Window T-test''\n",
    "}\n",
    "\n",
    "# 3. Bayesian Online Detection\n",
    "print(\"\\n3. BAYESIAN ONLINE DETECTION:\")\n",
    "print(f\"   • Number of change points: {len(detected_cp_indices)}\")\n",
    "print(f\"   • Top dates: {'', ''.join([dates_subset[i].date().strftime(''%Y-%m-%d'') for i in top_cp_indices[:3]])}\")\n",
    "\n",
    "model_results[''bayesian_online''] = {\n",
    "    ''n_changepoints'': len(detected_cp_indices),\n",
    "    ''locations'': [dates_subset[i] for i in detected_cp_indices],\n",
    "    ''method'': ''Bayesian Online''\n",
    "}\n",
    "\n",
    "# Compare model predictions\n",
    "print(\"\\n=== MODEL COMPARISON SUMMARY ===\")\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        ''Model'': ''Single Bayesian'',\n",
    "        ''Change Points'': 1,\n",
    "        ''Key Date'': dates[tau_mean].date(),\n",
    "        ''Mean Before'': f\"{np.mean(mu1_samples):.4f}%\",\n",
    "        ''Mean After'': f\"{np.mean(mu2_samples):.4f}%\",\n",
    "        ''Method'': ''MCMC Sampling'',\n",
    "        ''Strength'': ''Precise uncertainty quantification'',\n",
    "        ''Weakness'': ''Assumes only one change point''\n",
    "    },\n",
    "    {\n",
    "        ''Model'': ''Rolling Window'',\n",
    "        ''Change Points'': len(robust_cps),\n",
    "        ''Key Date'': cp_dates[0].date() if len(cp_dates) > 0 else ''N/A'',\n",
    "        ''Mean Before'': ''Multiple regimes'',\n",
    "        ''Mean After'': ''Multiple regimes'',\n",
    "        ''Method'': ''Statistical testing'',\n",
    "        ''Strength'': ''Detects multiple change points'',\n",
    "        ''Weakness'': ''Sensitive to window size''\n",
    "    },\n",
    "    {\n",
    "        ''Model'': ''Bayesian Online'',\n",
    "        ''Change Points'': len(detected_cp_indices),\n",
    "        ''Key Date'': dates_subset[detected_cp_indices[0]].date() if len(detected_cp_indices) > 0 else ''N/A'',\n",
    "        ''Mean Before'': ''Dynamic'',\n",
    "        ''Mean After'': ''Dynamic'',\n",
    "        ''Method'': ''Sequential Bayes'',\n",
    "        ''Strength'': ''Online, handles multiple changes'',\n",
    "        ''Weakness'': ''Computationally intensive''\n",
    "    }\n",
    "])\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "# Recommendation\n",
    "print(\"\\n=== RECOMMENDATION ===\")\n",
    "print(\"For this analysis, we recommend using the Rolling Window approach because:\")\n",
    "print(\"1. It detects multiple change points (more realistic for long time series)\")\n",
    "print(\"2. It's computationally efficient\")\n",
    "print(\"3. Results are interpretable and statistically validated\")\n",
    "print(\"4. It provides robust detection across multiple window sizes\")\n",
    "\n",
    "print(\"\\nThe Single Bayesian model provides excellent uncertainty quantification\")\n",
    "print(\"but is too restrictive for our 35-year dataset with multiple regimes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SAVING RESULTS ===\")\n",
    "\n",
    "# Prepare results for saving\n",
    "results = {\n",
    "    ''analysis_date'': pd.Timestamp.now(),\n",
    "    ''data_info'': {\n",
    "        ''n_observations'': n_obs,\n",
    "        ''date_range'': f\"{dates.min()} to {dates.max()}\",\n",
    "        ''mean_return'': float(y.mean()),\n",
    "        ''std_return'': float(y.std())\n",
    "    },\n",
    "    ''single_change_point'': {\n",
    "        ''date'': dates[tau_mean].strftime(''%Y-%m-%d''),\n",
    "        ''index'': int(tau_mean),\n",
    "        ''mean_before'': float(np.mean(mu1_samples)),\n",
    "        ''mean_after'': float(np.mean(mu2_samples)),\n",
    "        ''mean_difference'': float(np.mean(mu_diff)),\n",
    "        ''probability_mu2_gt_mu1'': float(prob_mu2_gt_mu1)\n",
    "    },\n",
    "    ''multiple_change_points'': {\n",
    "        ''n_points'': len(robust_cps),\n",
    "        ''dates'': [d.strftime(''%Y-%m-%d'') for d in cp_dates],\n",
    "        ''indices'': robust_cps\n",
    "    },\n",
    "    ''event_associations'': []\n",
    "}\n",
    "\n",
    "# Save event associations if available\n",
    "if ''assoc_df'' in locals() and not assoc_df.empty:\n",
    "    for _, row in assoc_df.iterrows():\n",
    "        results[''event_associations''].append({\n",
    "            ''change_point_date'': row[''change_point_date''].strftime(''%Y-%m-%d''),\n",
    "            ''event_name'': row[''event_name''],\n",
    "            ''event_date'': row[''event_date''].strftime(''%Y-%m-%d''),\n",
    "            ''days_difference'': int(row[''days_difference'']),\n",
    "            ''event_type'': row[''event_type''],\n",
    "            ''impact_level'': row[''impact_level''],\n",
    "            ''mean_change'': float(row.get(''mean_change'', 0)),\n",
    "            ''vol_change'': float(row.get(''vol_change'', 0))\n",
    "        })\n",
    "\n",
    "# Save to JSON\n",
    "import json\n",
    "with open(''../data/processed/change_point_results.json'', ''w'') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "# Save to CSV for easier analysis\n",
    "if ''assoc_df'' in locals() and not assoc_df.empty:\n",
    "    assoc_df.to_csv(''../data/processed/event_change_associations.csv'', index=False)\n",
    "\n",
    "# Save regime statistics\n",
    "regime_stats = []\n",
    "if len(robust_cps) >= 2:\n",
    "    regime_boundaries = [dates[0]] + cp_dates + [dates[-1]]\n",
    "    for i in range(len(regime_boundaries)-1):\n",
    "        start_idx = np.where(dates >= regime_boundaries[i])[0][0]\n",
    "        end_idx = np.where(dates <= regime_boundaries[i+1])[0][-1]\n",
    "        \n",
    "        regime_data = y[start_idx:end_idx]\n",
    "        if len(regime_data) > 10:\n",
    "            regime_stats.append({\n",
    "                ''regime_id'': i+1,\n",
    "                ''start_date'': regime_boundaries[i].strftime(''%Y-%m-%d''),\n",
    "                ''end_date'': regime_boundaries[i+1].strftime(''%Y-%m-%d''),\n",
    "                ''duration_days'': (regime_boundaries[i+1] - regime_boundaries[i]).days,\n",
    "                ''mean_return'': float(regime_data.mean()),\n",
    "                ''std_return'': float(regime_data.std()),\n",
    "                ''n_observations'': len(regime_data)\n",
    "            })\n",
    "    \n",
    "    regime_df = pd.DataFrame(regime_stats)\n",
    "    regime_df.to_csv(''../data/processed/regime_statistics.csv'', index=False)\n",
    "\n",
    "print(\"\\n✓ Results saved to:\")\n",
    "print(\"   • ../data/processed/change_point_results.json\")\n",
    "print(\"   • ../data/processed/event_change_associations.csv\")\n",
    "print(\"   • ../data/processed/regime_statistics.csv\")\n",
    "\n",
    "print(\"\\n=== TASK 2 COMPLETED SUCCESSFULLY ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== KEY FINDINGS FROM TASK 2 ===\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"1. CHANGE POINT DETECTION:\")\n",
    "print(f\"   • Single Bayesian model identified a major change point at: {dates[tau_mean].date()}\")\n",
    "print(f\"   • Rolling window analysis detected {len(robust_cps)} robust change points\")\n",
    "print(f\"   • Bayesian online detection found {len(detected_cp_indices)} change points in subset\")\n",
    "\n",
    "print(\"\\n2. EVENT ASSOCIATION:\")\n",
    "if ''assoc_df'' in locals() and not assoc_df.empty:\n",
    "    print(f\"   • {len(assoc_df)} change points associated with geopolitical events\")\n",
    "    print(f\"   • Average days between event and change point: {assoc_df[''days_difference''].mean():.1f}\")\n",
    "    \n",
    "    # Most frequent event type\n",
    "    most_common_type = assoc_df[''event_type''].mode()[0] if not assoc_df[''event_type''].mode().empty else ''N/A''\n",
    "    print(f\"   • Most associated event type: {most_common_type}\")\n",
    "else:\n",
    "    print(\"   • No strong associations found within 30-day window\")\n",
    "\n",
    "print(\"\\n3. IMPACT QUANTIFICATION:\")\n",
    "if ''assoc_df'' in locals() and not assoc_df.empty:\n",
    "    avg_mean_change = assoc_df[''mean_change''].mean()\n",
    "    avg_vol_change = assoc_df[''vol_change''].mean()\n",
    "    print(f\"   • Average mean return change: {avg_mean_change:+.3f}%\")\n",
    "    print(f\"   • Average volatility change: {avg_vol_change:+.3f}%\")\n",
    "    \n",
    "    # Most impactful event\n",
    "    if not assoc_df.empty:\n",
    "        most_impactful = assoc_df.loc[assoc_df[''mean_change''].abs().idxmax()]\n",
    "        print(f\"   • Most impactful event: {most_impactful[''event_name'']} ({most_impactful[''mean_change'']:+.3f}%)\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS FOR STAKEHOLDERS:\")\n",
    "print(\"   • Investors: Monitor for change points as regime shift indicators\")\n",
    "print(\"   • Policymakers: Recognize delayed market responses to geopolitical events\")\n",
    "print(\"   • Analysts: Use rolling window approach for robust change point detection\")\n",
    "print(\"   • Risk Managers: Adjust volatility models during detected high-volatility regimes\")\n",
    "\n",
    "print(\"\\n5. LIMITATIONS AND FUTURE WORK:\")\n",
    "print(\"   • Correlation vs. Causation: Associations don't prove causality\")\n",
    "print(\"   • Model Complexity: More sophisticated models could capture gradual transitions\")\n",
    "print(\"   • External Factors: Other economic variables not included\")\n",
    "print(\"   • Event Timing: Market anticipation may precede official event dates\")\n",
    "\n",
    "print(\"\\n=== READY FOR TASK 3: DASHBOARD DEVELOPMENT ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
